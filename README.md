# MyGenAI
## Introduction
This repository serves as a comprehensive guide and workspace for learning Generative AI. It includes detailed documentation, Python programs, and practical examples to explore and understand the concepts of Generative AI. The goal is to document learnings, experiment with models, and build interactive AI applications using tools like Google Vertex AI and Python.

## Pre-Requsites
* Ubuntu or WSL environment
    [Install Linux on Windows with WSL](https://learn.microsoft.com/en-us/windows/wsl/install)
* Install Python 3.12 or higher version
 

## Setup Python Virtual Environment

### Create python virtual environment
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### Install requirements
```bash
(.venv) $ pip install -r requirements.txt
```

## Setup GCP Environment

### Create a Google Cloud Project
[Link](https://console.cloud.google.com/projectselector2/home/dashboard) to create the Project

Refer [How To Guide](https://developers.google.com/workspace/guides/create-project#google-cloud-console)

### Create a Billing Account
[Link](https://console.cloud.google.com/billing?organizationId=0) to create billing account.

### Enable Vertex APIs in Google cloud
You can refer the following guide to enable APIs. Search for `VertexAI` and enable it 
[Enable APIs](https://cloud.google.com/endpoints/docs/openapi/enable-api#console)



### Install gcloud cli
```bash
sudo apt update
sudo apt install apt-transport-https ca-certificates gnupg curl -y

echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" | sudo tee /etc/apt/sources.list.d/google-cloud-sdk.list

curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg

sudo apt update
sudo apt install google-cloud-sdk -y
```

### Initialize and Set Region/Zone
```bash
gcloud init # Authenticates,updated project configuration
gcloud config set project PROJECT_ID  # Refer the Project ID under project in GCP
gcloud config set compute/region us-central1 # Set the region

gcloud config set compute/zone us-central1-a # And Zone

gcloud config list # View default GCP configuration
```

Verify if VertexAI API is enabled
```bash
gcloud services list --enabled --filter="config.name=aiplatform.googleapis.com"
```

## Sample GenAI program
Set Environment
```bash
export GOOGLE_CLOUD_PROJECT="<PROJECT_ID>"
export GOOGLE_CLOUD_LOCATION="us-central1"
export GOOGLE_GENAI_USE_VERTEXAI=True
```
Run Program
```bash
(.venv) saiky@Saikumar:~/GenAI$ python3 ./src/first_genai.py 
The current Prime Minister of India is Narendra Modi.
```


## Sample Interactive AI Program
```bash
(.venv) saiky@Saikumar:~/GenAI$ python3 src/first_vertexai.py 
Mr. LLM: Ask your query!!
         or 'quit' to exit: 
You: Hello There!!!
Mr LLM : Hello! How can I help you today? ðŸ˜Š

You : Tell me about Generative AI in 3 lines
Mr LLM : Generative AI creates new content, like text, images, audio, or code, based on learned patterns from existing data.  It leverages algorithms to "understand" the data and then generate novel outputs that resemble it.  Examples include writing articles, designing logos, composing music, and even developing software.

You : Which version of LLM are you using ?
Mr LLM : I am currently running on the Gemini Pro model.

You : Thanks
Mr LLM : You're welcome! Is there anything specific I can help you with today? Let me know what you're thinking or what you need assistance with.

You : quit
```

# LLM response

```json
{
  // Top-level object containing the entire response
  "candidates": [ // An array potentially containing multiple response options (candidates) from the model. Usually, you'll get one unless you request more.
    {
      // Represents a single response candidate (the first and only one in this case)
      "avg_logprobs": -0.018483588641340084, // An advanced metric indicating the model's average confidence per token in this generated sequence. Log probabilities are typically negative; values closer to 0 indicate higher confidence. Often used for analysis rather than basic interaction.
      "content": { // The actual content generated by the model for this candidate.
        "parts": [ // An array containing parts of the content. This structure supports multi-modal responses (text, images, function calls etc.). For simple text, it usually has one part.
          {
            "text": "The current Prime Minister of India is Narendra Modi.\n" // The actual text string generated by the model. This is usually the main piece of information you want.
          }
        ],
        "role": "model" // Indicates who generated this content. 'model' means it came from the LLM. In conversational context, your input parts would have the role 'user'.
      },
      "finish_reason": "STOP" // Explains why the model stopped generating text for this candidate. Common reasons:
                               // - "STOP": The model naturally concluded its response.
                               // - "MAX_TOKENS": Generation stopped because it reached the maximum output token limit you set.
                               // - "SAFETY": Generation stopped due to safety filters detecting potentially harmful content in the prompt or response.
                               // - "RECITATION": Generation stopped because it detected content matching copyrighted material.
                               // - "OTHER": Stopped for other reasons.
    }
  ],
  "create_time": "2025-05-01T14:23:17.419503Z", // The timestamp (in UTC) when this specific response object was created by the backend service. 
  "model_version": "gemini-2.0-flash-001", // The specific version of the Gemini model that processed your request and generated the response. Useful for tracking model behavior over time.
  "response_id": "1YMTaK_NGYyDqsMPqp2B-Ac", // A unique identifier for this specific API response. Useful for logging, debugging, or if you need to reference this particular interaction with Google Cloud support.
  "usage_metadata": { // Information about the resources consumed by this API call, primarily token counts. Important for understanding costs and limits.
    "candidates_token_count": 11, // The total number of tokens in the generated response content across all candidates (in this case, just the one candidate's text).
    "candidates_tokens_details": [ // A breakdown of the candidate tokens, often by modality (like TEXT). Useful for multi-modal responses.
      {
        "modality": "TEXT", // The type of data the tokens represent.
        "token_count": 11 // The number of tokens for this specific modality in the candidates.
      }
    ],
    "prompt_token_count": 8, // The total number of tokens in the input prompt you sent to the model.
    "prompt_tokens_details": [ // A breakdown of the prompt tokens, often by modality.
      {
        "modality": "TEXT", // The type of data the tokens represent in the prompt.
        "token_count": 8 // The number of tokens for this specific modality in the prompt.
      }
    ],
    "total_token_count": 19 // The sum of `prompt_token_count` and `candidates_token_count`. This is often the key number used for billing purposes.
  }
}
```